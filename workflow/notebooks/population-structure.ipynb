{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01954f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "\n",
    "def plot_pca(pca_df, colour_column, cohort_columns, dataset,  x='PC1',y='PC2',z='PC3', color_mapping=None, height=500, width=750):\n",
    "    fig= px.scatter_3d(\n",
    "        pca_df, \n",
    "        x=x, \n",
    "        y=y,\n",
    "        z=z, \n",
    "        title=f\"PCA {dataset} | PC1 vs PC2 vs PC3 coloured by {colour_column}\",\n",
    "        color=colour_column, \n",
    "        hover_data=cohort_columns + ['sample_id'],\n",
    "        color_discrete_map=color_mapping[colour_column], \n",
    "        template='simple_white',\n",
    "        height=height,\n",
    "        width=width\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6302ed5",
   "metadata": {
    "tags": [
     "parameters",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = 'ag-vampir-002'\n",
    "vcf_path = f\"../../results/vcfs/amplicons/{dataset}.annot.vcf\"\n",
    "metadata_path = \"../../results/config/metadata.qcpass.tsv\"\n",
    "cohort_cols = 'location,taxon'\n",
    "wkdir = \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72254713",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(wkdir, 'workflow'))\n",
    "import ampseekertools as amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696938a0",
   "metadata": {},
   "source": [
    "## Population structure\n",
    "\n",
    "In this notebook, we run a principal components analysis and build a neighbour joining tree on the amplicon sequencing variant data. For the PCA, we will plot PC1 v PC2 and PC3 v PC4, and the variance explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae6392",
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "cohort_cols = cohort_cols.split(\",\")\n",
    "\n",
    "if metadata_path.endswith('.xlsx'):\n",
    "    metadata = pd.read_excel(metadata_path, engine='openpyxl')\n",
    "elif metadata_path.endswith('.tsv'):\n",
    "    metadata = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "elif metadata_path.endswith('.csv'):\n",
    "    metadata = pd.read_csv(metadata_path, sep=\",\")\n",
    "else:\n",
    "    raise ValueError(\"Metadata file must be .xlsx or .csv\")\n",
    "\n",
    "import json\n",
    "with open(f\"{wkdir}/config/metadata_colours.json\", 'r') as f:\n",
    "    color_mapping = json.load(f)\n",
    "\n",
    "geno, pos, contig, metadata, ref, alt, ann = amp.load_vcf(vcf_path, metadata)\n",
    "df_pca, model = amp.pca(geno=geno, metadata=metadata, n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b018edf",
   "metadata": {},
   "source": [
    "#### Variance explained\n",
    "\n",
    "As a general rule of thumb, when the variance explained for each PC begins to flatten out, that is when the PCs are no longer informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a0d44",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = px.bar(model.explained_variance_ratio_ , labels={\n",
    "                     \"value\": \"Variance Explained\",\n",
    "                     \"index\": \"Principal Component\",\n",
    "                 }, template='simple_white', height=250, width=600)\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b98cf",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e807c",
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "for coh in cohort_cols:\n",
    "    fig1 = plot_pca(df_pca, x='PC1',y='PC2',z='PC3', colour_column=coh, cohort_columns=cohort_cols, dataset=dataset, color_mapping=color_mapping)\n",
    "    fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbe3f6",
   "metadata": {},
   "source": [
    "## NJT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53849e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from scipy.spatial.distance import squareform  # type: ignore\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def multiallelic_diplotype_pdist(X, metric):\n",
    "    \"\"\"Optimised implementation of pairwise distance between diplotypes.\n",
    "\n",
    "    N.B., here we assume the array X provides diplotypes as genotype allele\n",
    "    counts, with axes in the order (n_samples, n_sites, n_alleles).\n",
    "\n",
    "    Computation will be faster if X is a contiguous (C order) array.\n",
    "\n",
    "    The metric argument is the function to compute distance for a pair of\n",
    "    diplotypes. This can be a numba jitted function.\n",
    "\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_pairs = (n_samples * (n_samples - 1)) // 2\n",
    "    out = np.zeros(n_pairs, dtype=np.float32)\n",
    "\n",
    "    # Loop over samples, first in pair.\n",
    "    for i in range(n_samples):\n",
    "        x = X[i, :, :]\n",
    "\n",
    "        # Loop over observations again, second in pair.\n",
    "        for j in numba.prange(i + 1, n_samples):\n",
    "            y = X[j, :, :]\n",
    "\n",
    "            # Compute distance for the current pair.\n",
    "            d = metric(x, y)\n",
    "\n",
    "            # Store result for the current pair.\n",
    "            k = square_to_condensed(i, j, n_samples)\n",
    "            out[k] = d\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def square_to_condensed(i, j, n):\n",
    "    \"\"\"Convert distance matrix coordinates from square form (i, j) to condensed form.\"\"\"\n",
    "\n",
    "    assert i != j, \"no diagonal elements in condensed matrix\"\n",
    "    if i < j:\n",
    "        i, j = j, i\n",
    "    return n * j - j * (j + 1) // 2 + i - 1 - j\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def multiallelic_diplotype_mean_cityblock(x, y):\n",
    "    \"\"\"Compute the mean cityblock distance between two diplotypes x and y. The\n",
    "    diplotype vectors are expected as genotype allele counts, i.e., x and y\n",
    "    should have the same shape (n_sites, n_alleles).\n",
    "\n",
    "    N.B., here we compute the mean value of the distance over sites where\n",
    "    both individuals have a called genotype. This avoids computing distance\n",
    "    at missing sites.\n",
    "\n",
    "    \"\"\"\n",
    "    n_sites = x.shape[0]\n",
    "    n_alleles = x.shape[1]\n",
    "    distance = np.float32(0)\n",
    "    n_sites_called = np.float32(0)\n",
    "\n",
    "    # Loop over sites.\n",
    "    for i in range(n_sites):\n",
    "        x_is_called = False\n",
    "        y_is_called = False\n",
    "        d = np.float32(0)\n",
    "\n",
    "        # Loop over alleles.\n",
    "        for j in range(n_alleles):\n",
    "            # Access allele counts.\n",
    "            xc = np.float32(x[i, j])\n",
    "            yc = np.float32(y[i, j])\n",
    "\n",
    "            # Check if any alleles observed.\n",
    "            x_is_called = x_is_called or (xc > 0)\n",
    "            y_is_called = y_is_called or (yc > 0)\n",
    "\n",
    "            # Compute cityblock distance (absolute difference).\n",
    "            d += np.fabs(xc - yc)\n",
    "\n",
    "        # Accumulate distance for the current pair, but only if both samples\n",
    "        # have a called genotype.\n",
    "        if x_is_called and y_is_called:\n",
    "            distance += d\n",
    "            n_sites_called += np.float32(1)\n",
    "\n",
    "    # Compute the mean distance over sites with called genotypes.\n",
    "    if n_sites_called > 0:\n",
    "        mean_distance = distance / n_sites_called\n",
    "    else:\n",
    "        mean_distance = np.nan\n",
    "\n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kdr_origins = pd.read_csv(f\"{wkdir}/results/kdr-origins/kdr_origins.csv\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e769f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kdr_origins = df_kdr_origins.reset_index().rename(columns={'index':'sample_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0977f1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import anjl\n",
    "\n",
    "ac = allel.GenotypeArray(gn_seg).to_allele_counts(max_allele=3)\n",
    "X = np.ascontiguousarray(np.swapaxes(ac.values, 0, 1))\n",
    "\n",
    "dists = multiallelic_diplotype_pdist(X, metric=multiallelic_diplotype_mean_cityblock)\n",
    "dists = squareform(dists)\n",
    "df_samples = metadata.set_index('sample_id')\n",
    "df = df_samples[['taxon', 'location']]\n",
    "\n",
    "df_dist_matrix = pd.DataFrame(dists, index=df_samples.index.to_list(), columns=df_samples.index.to_list())\n",
    "# pivot long \n",
    "df_dists = df_dist_matrix.stack().reset_index().set_axis('sample_id_x sample_id_y distance'.split(), axis=1)\n",
    "# merge with metadata\n",
    "df_dists = df_dists.merge(df, left_on='sample_id_x', right_index=True).merge(df, left_on='sample_id_y', right_index=True, suffixes=('_x', '_y'))\n",
    "# remove self comparisons\n",
    "df_dists = df_dists[df_dists['sample_id_x'] != df_dists['sample_id_y']]\n",
    "# dedup\n",
    "df_dists = df_dists.assign(dedup=np.array([''.join(sorted([a,b])) for a,b in zip(df_dists.sample_id_x, df_dists.sample_id_y)]).astype(str))\n",
    "df_dists = df_dists.sort_values('sample_id_x').drop_duplicates('dedup').drop('dedup', axis=1)\n",
    "# normalise distances\n",
    "df_dists = df_dists.assign(location=lambda x: x.location_x + \" | \" + x.location_y).drop(['location_x', 'location_y'], axis=1)\n",
    "df_grp_dists = df_dists.groupby('location').agg({'distance': 'mean'}).sort_values('distance').rename(columns={'distance': 'mean_distance'}).reset_index()\n",
    "df_dists = df_dists.merge(df_grp_dists, on='location').assign(normalised_dist=lambda x: x.distance - x.mean_distance).sort_values('normalised_dist')\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# get the 500 most distant samples and exclude highly irregular ones \n",
    "far_samples = df_dists.sort_values('normalised_dist', ascending=False)[:int(df_dists.shape[0] * 0.005)][['sample_id_x', 'sample_id_y']].values.flatten()\n",
    "far_samples, far_counts = np.unique(far_samples, return_counts=True)\n",
    "exclude_outliers = far_samples[far_counts > int(df_samples.shape[0] * 0.1)]\n",
    "print(f\"excluding extreme outliers from NJT\", exclude_outliers)\n",
    "\n",
    "dists = df_dist_matrix.drop(exclude_outliers, axis=0).drop(exclude_outliers, axis=1).values\n",
    "leaf_data = df_samples.query(\"sample_id not in @exclude_outliers\").reset_index()\n",
    "leaf_data = leaf_data.merge(df_kdr_origins[['sample_id', 'kdr_origin']], on='sample_id', how='left')\n",
    "\n",
    "Z = anjl.dynamic_nj(dists)\n",
    "\n",
    "for col in cohort_cols:\n",
    "    fig = anjl.plot(\n",
    "        Z,\n",
    "        leaf_data=leaf_data,\n",
    "        color=col,\n",
    "        hover_name=\"sample_id\",\n",
    "        hover_data=cohort_cols + ['kdr_origin'],  \n",
    "        color_discrete_map=color_mapping[col],\n",
    "        marker_size=8\n",
    "    )\n",
    "    fig.write_image(f\"{wkdir}/results/njt_{col}.png\", scale=2)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac9a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
